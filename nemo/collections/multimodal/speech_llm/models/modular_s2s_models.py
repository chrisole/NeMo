import itertools
import json
import math
import os
import re
import tempfile
from collections import OrderedDict
from typing import List, Optional, Union

import hydra
import numpy as np
import sacrebleu
import soundfile as sf
import torch
import torchaudio
from omegaconf import DictConfig, OmegaConf
from omegaconf.omegaconf import OmegaConf, open_dict
from pytorch_lightning.trainer.trainer import Trainer
from pytorch_lightning.utilities import rank_zero_only
from torch import Tensor
from torch.nn.utils.rnn import pad_sequence
from torchaudio.pipelines import SQUIM_SUBJECTIVE
from nemo.collections.multimodal.speech_llm.modules.streaming_transformer import StreamingTransformer
from nemo.collections.asr.parts.utils.eval_utils import remove_punctuations
from nemo.collections.common.metrics import MetricStringToTorchMetric, TextMetricsSet
from nemo.collections.common.parts.utils import apply_rope_scaling, extend_instance
from nemo.collections.multimodal.speech_llm.models.modular_models import ModularAudioGPTModel
from nemo.collections.nlp.models.language_modeling.megatron_gpt_model import EmbeddingScalingMixin, get_specs
from nemo.collections.nlp.models.language_modeling.megatron_gpt_sft_model import MegatronGPTSFTModel
from nemo.collections.nlp.parts.nlp_overrides import NLPSaveRestoreConnector
from nemo.collections.nlp.parts.utils_funcs import get_last_rank
from nemo.utils import AppState, logging, model_utils

try:
    from megatron.core import InferenceParams, parallel_state, tensor_parallel
    from megatron.core.models.common.embeddings.language_model_embedding import LanguageModelEmbedding
    from megatron.core.models.gpt import GPTModel as MCoreGPTModel
    from megatron.core.transformer.transformer_config import TransformerConfig

    try:
        from megatron.core.num_microbatches_calculator import (
            get_num_microbatches,
            reconfigure_num_microbatches_calculator,
        )

    except (ImportError, ModuleNotFoundError):
        logging.warning("Megatron num_microbatches_calculator not found, using Apex version.")
        from apex.transformer.pipeline_parallel.utils import (
            _reconfigure_microbatch_calculator as reconfigure_num_microbatches_calculator,
        )
        from apex.transformer.pipeline_parallel.utils import get_num_microbatches
    from megatron.core.packed_seq_params import PackedSeqParams

    HAVE_MEGATRON_CORE = True

except (ImportError, ModuleNotFoundError):
    HAVE_MEGATRON_CORE = False

default_inference_config = {'tokens_to_generate': 30}


def sample_token(
    logits: torch.Tensor,
    use_sampling: bool = False,
    temp: float = 1.0,
) -> torch.Tensor:

    if use_sampling and temp > 0.0:
        probs = torch.softmax(logits / temp, dim=-1)
        next_token = multinomial(probs, num_samples=1)
    else:
        next_token = torch.argmax(logits, dim=-1, keepdim=True)
    assert next_token.shape[-1] == 1
    return next_token[..., 0]


class SumVocabParallelEmbedding(tensor_parallel.VocabParallelEmbedding):

    def __init__(
        self,
        proj_head_dims,
        *args,
        **kwargs,
    ) -> None:
        super().__init__(*args, **kwargs)
        self.proj_head_dims = proj_head_dims

    def forward(self, input_):

        if input_.ndim == 3:
            assert input_.shape[2] == len(self.proj_head_dims)
            input_ = input_.clone()
            for i in range(len(self.proj_head_dims)):
                # shuold consider the offset of previous projection heads
                input_[:, :, i] += sum(self.proj_head_dims[:i])
            assert input_.max() < sum(self.proj_head_dims)
        embeddings = super().forward(input_)
        if input_.ndim == 3:
            # sum the multi proj embeddings as the final embeddings
            embeddings = torch.sum(embeddings, axis=2)
        return embeddings


class SumMultiEmbedding(LanguageModelEmbedding):
    """Language model embeddings with multiple tokens at each time step. The embeddings of the tokens of the same time step will be computed separately and then be summed together."""

    def __init__(
        self,
        proj_head_dims,
        *args,
        **kwargs,
    ) -> None:
        super().__init__(*args, **kwargs)
        del self.word_embeddings
        self.word_embeddings = SumVocabParallelEmbedding(
            num_embeddings=self.vocab_size,
            embedding_dim=self.config.hidden_size,
            init_method=self.config.init_method,
            reduce_scatter_embeddings=self.reduce_scatter_embeddings,
            config=self.config,
            proj_head_dims=proj_head_dims,
        )


class S2sMCoreGPTModel(MCoreGPTModel):

    def __init__(
        self,
        config: TransformerConfig,
        proj_head_dims: List[int],
        proj_head_loss_weights: List[float],
        *args,
        **kwargs,
    ) -> None:
        super().__init__(config=config, *args, **kwargs)
        self.n_proj_heads = len(proj_head_dims)
        self.proj_head_dims = proj_head_dims
        self.proj_head_loss_weights = proj_head_loss_weights

        # (512,8,16) make the depformer para. to be 105M
        self.depformer_dim = 512
        self.depformer = StreamingTransformer(
            d_model=self.depformer_dim,
            dim_feedforward=int(self.depformer_dim * 4),
            norm="layer_norm",
            num_heads=8,
            num_layers=16
            # device=self.device,
        )

        # One linear layer per codebook to project different informations from the main model.
        self.depformer_in = torch.nn.ModuleList(
            [torch.nn.Linear(config.hidden_size, self.depformer_dim, bias=False) for _ in range(self.n_proj_heads - 1)]
        )
        # Only using up to dep_q - 1 because the last codebook is never an input to Depformer.
        self.depformer_emb = torch.nn.ModuleList(
            [torch.nn.Embedding(self.proj_head_dims[i+1], self.depformer_dim) for i in range(self.n_proj_heads - 2)]
        )
        self.depformer_text_emb = torch.nn.Embedding(self.proj_head_dims[0], self.depformer_dim)

        self.depformer.set_streaming_propagate(False)

        # This liner is map the depformer output to audio voca.
        self.linears = torch.nn.ModuleList(
            [torch.nn.Linear(self.depformer_dim, self.proj_head_dims[1], bias=False) for _ in range(self.n_proj_heads - 1)]
        )

    def extend_embedding(self, vocab_size: int):
        """Extend the embedding layer with new vocab size."""

        # Extend word embedding table if self.padded_vocab_size is larger than the size of the pre-trained word embedding
        pretrained_emb = self.embedding

        self.embedding = SumMultiEmbedding(
            config=self.config,
            vocab_size=vocab_size,
            max_sequence_length=self.max_sequence_length,
            position_embedding_type=self.position_embedding_type,
            proj_head_dims=self.proj_head_dims,
        )
        self.embedding.word_embeddings.weight.data[: pretrained_emb.word_embeddings.weight.shape[0]] = (
            pretrained_emb.word_embeddings.weight.data
        )
        # Zero out the new embeddings to make the model behave the same as it was pre-trained
        self.embedding.word_embeddings.weight.data[pretrained_emb.word_embeddings.weight.shape[0] :].zero_()
        del pretrained_emb
        if self.pre_process or self.post_process:
            self.setup_embeddings_and_output_layer()



    def forward(
        self,
        input_ids: Tensor,
        position_ids: Tensor,
        attention_mask: Tensor,
        decoder_input: Tensor = None,
        labels: Tensor = None,
        inference_params: InferenceParams = None,
        packed_seq_params: PackedSeqParams = None,
        extra_block_kwargs: dict = None,
    ) -> Tensor:
        """Forward function of the GPT Model This function passes the input tensors
        through the embedding layer, and then the decoeder and finally into the post
        processing layer (optional).

        It either returns the Loss values if labels are given  or the final hidden units
        """
        # If decoder_input is provided (not None), then input_ids and position_ids are ignored.
        # Otherwise, apply embedding layer on input_ids and position_ids to get decoder_input.

        # Decoder embedding

        if decoder_input is not None:
            pass
        elif self.pre_process:
            decoder_input = self.embedding(input_ids=input_ids, position_ids=position_ids)
        else:
            # intermediate stage of pipeline
            # decoder will get hidden_states from encoder.input_tensor
            decoder_input = None

        # Rotary positional embeddings (embedding is None for PP intermediate devices)
        rotary_pos_emb = None
        if self.position_embedding_type == 'rope':
            rotary_seq_len = self.rotary_pos_emb.get_rotary_seq_len(
                inference_params, self.decoder, decoder_input, self.config
            )
            rotary_pos_emb = self.rotary_pos_emb(rotary_seq_len)

        # Run decoder.
        hidden_states = self.decoder(
            hidden_states=decoder_input,
            attention_mask=attention_mask,
            inference_params=inference_params,
            rotary_pos_emb=rotary_pos_emb,
            packed_seq_params=packed_seq_params,
            **(extra_block_kwargs or {}),
        )
        if not self.post_process:
            return hidden_states

        if self.share_embeddings_and_output_weights:
            output_weight = self.shared_embedding_or_output_weight()
        else:
            output_weight = None
        # Here the text logits is calculated separately
        text_logits, _ = self.output_layer(
            hidden_states, weight=output_weight[: self.vocab_size] if output_weight is not None else None
        )



        if labels is not None:

            # Training Mode:
            # reshape the hidden_state: [B, T, D] -> [B*T, 1, D]
            depformer_hidden_states = hidden_states.reshape(-1, 1, self.config.hidden_size)
            # Format depfomer input with teacher-forcing: depformer_input = [B * T, 8, 1], 8 is remove the last audio channel.
            depformer_input = labels[:, :, : self.n_proj_heads -1].reshape(-1, self.n_proj_heads -1, 1)

            # depformer_input_layers downsample the hidden_states to depformer dim
            depformer_input_layers = torch.nn.ModuleList([self.depformer_in[cb_index] for cb_index in range(self.n_proj_heads -1)])
            # [B*T, 1, D] ==> [B*T, 1, D_dep]
            depformer_hidden_states = torch.stack([in_layer(depformer_hidden_states) for in_layer in depformer_input_layers], dim=1).squeeze(2)

            # depformer_emb_layer is to process the input from label
            depformer_emb_layers = torch.nn.ModuleList(
                [self.depformer_text_emb] + [self.depformer_emb[cb_index] for cb_index in range(self.n_proj_heads -2)])

            last_token_input = torch.stack([emb_layer(depformer_input[:, i]) for i, emb_layer in enumerate(depformer_emb_layers)],
                                           dim=1).squeeze(2)

            depformer_linear_layers = torch.nn.ModuleList([self.linears[cb_index] for cb_index in range(self.n_proj_heads -1)])

            depformer_hidden_states = depformer_hidden_states + last_token_input

            depformer_output = self.depformer(depformer_hidden_states)

            all_logits = [text_logits] # [T, B, V]
            for i, audio_linear_layer in enumerate(depformer_linear_layers):
                all_logits.append(audio_linear_layer(depformer_output[:, i, :]).reshape(text_logits.shape[0], text_logits.shape[1], -1))
            tokens_loss = torch.stack(
                [self.compute_language_model_loss(labels[:, :, i], all_logits[i]) for i in range(self.n_proj_heads)],
                axis=2,
            )
            tokens_loss = (
                tokens_loss
                * torch.FloatTensor(self.proj_head_loss_weights).to(tokens_loss.device)
                / sum(self.proj_head_loss_weights)
            )
            return tokens_loss

        else:

            # We need first get text token to compute audio logits.
            # TODO: Here simple greedy strategy is used, which is different with real decoding strategy
            text_token = sample_token(text_logits) # [1, B]

            hidden_states = hidden_states.reshape(-1, 1, self.config.hidden_size)# [T*B, D]

            # with self.depformer.streaming(S):
            all_logits = [text_logits.transpose(0, 1)] #[1, B, V] --> [B, 1, V]

            for cb_index in range(self.n_proj_heads -1):

                depformer_hidden_states = self.depformer_in[cb_index](hidden_states) # [B, 1, D_dep], not sure why second dim always be 1.

                if cb_index == 0:
                    last_token_input = self.depformer_text_emb(text_token.transpose(0,1))  # [B, 1, D_dep]
                else:
                    last_token_input = self.depformer_emb[cb_index - 1](
                        text_token
                    )

                depformer_hidden_states = depformer_hidden_states + last_token_input

                depformer_output = self.depformer(depformer_hidden_states)  # [B, 1, depformer_dim]

                logits = self.linears[cb_index](depformer_output) # [B, 1, V_dep]
                all_logits.append(logits)

                next_token = sample_token(logits)

                # now the new generated audio_tokne becomes new text_token
                text_token = next_token
            return torch.cat(all_logits, dim=-1)
            #return torch.empty(text_logits.shape[1],text_logits.shape[0], 64296).uniform_(-0.1, 0.1).to(text_logits.device)



class S2sModularAudioGPTModel(ModularAudioGPTModel):
    """S2S version of Modularized speech GPT model."""

    def model_provider_func(self, pre_process, post_process):
        """Model depends on pipeline paralellism."""
        if self.mcore_gpt:
            if not hasattr(self.cfg, 'decoder_reduction_factor'):
                self.decoder_reduction_factor = 1
            else:
                self.decoder_reduction_factor = self.cfg.decoder_reduction_factor
            self.proj_head_dims = self.cfg.proj_head_dims
            self.proj_head_loss_weights = self.cfg.get('proj_head_loss_weights', [1.0])
            if self.decoder_reduction_factor != 1:
                self.proj_head_dims = [self.proj_head_dims[0]] + self.proj_head_dims[
                    1:
                ] * self.decoder_reduction_factor
                self.proj_head_loss_weights = [self.cfg.proj_head_loss_weights[0]] + self.cfg.proj_head_loss_weights[
                    1:
                ] * self.decoder_reduction_factor

            model = S2sMCoreGPTModel(
                config=self.transformer_config,
                transformer_layer_spec=get_specs(
                    self.spec_name,
                    self.transformer_config,
                    self.transformer_engine,
                    self.cfg.get('hyena', None),
                ),
                vocab_size=self.padded_vocab_size,  # later can be updated to s2s_vocab_size
                max_sequence_length=self.cfg.get('encoder_seq_length', 512),
                pre_process=pre_process,
                post_process=post_process,
                parallel_output=True,
                share_embeddings_and_output_weights=self.cfg.get('share_embeddings_and_output_weights', True),
                position_embedding_type=self.cfg.get('position_embedding_type', 'learned_absolute'),
                rotary_percent=self.cfg.get('rotary_percentage', 1.0),
                seq_len_interpolation_factor=self.cfg.get('seq_len_interpolation_factor', None),
                rotary_base=self.cfg.get('rotary_base', 10000),
                proj_head_dims=self.proj_head_dims,
                proj_head_loss_weights=self.proj_head_loss_weights,
            )

            if self.cfg.get('scale_positional_embedding', False):
                model.rotary_pos_emb.inv_freq = apply_rope_scaling(model.rotary_pos_emb.inv_freq)

            if self.cfg.get("apply_embedding_scaling", False) and parallel_state.is_pipeline_first_stage():
                extend_instance(model.embedding, EmbeddingScalingMixin)
        else:
            raise ValueError("S2S ModularAudioGPTModel requires Megatron-core GPT model.")
        return model

    @classmethod
    def restore_from_pretrained_models(
        cls,
        cfg: Optional[Union[OmegaConf, str]] = None,
        trainer: Optional[Trainer] = None,
    ):

        model = super().restore_from_pretrained_models(cfg, trainer)

        codec_model, codec_model_cfg = cls.get_codec_models_and_configs(cfg)
        logging.info(f"Loaded Codec Model: {codec_model}")

        asr_model, asr_model_cfg = cls.get_asr_models_and_configs(cfg)
        logging.info(f"Loaded ASR Model: {asr_model}")

        mos_model = cls.get_mos_models_and_configs(cfg)
        logging.info(f"Loaded MOS Model: {mos_model}")

        if cfg.model.get('salm_model_path') is not None:
            # this may only work for tp=1
            # check scripts/nlp_language_modeling/merge_lora_weights/merge_salm.py on tp>1
            salm_model_path = cfg.model.get('salm_model_path')
            if '.nemo' in salm_model_path:
                with tempfile.TemporaryDirectory() as tmpdir:
                    NLPSaveRestoreConnector._unpack_nemo_file(salm_model_path, tmpdir)
                    salm_model_path = f"{tmpdir}/model_weights.ckpt"
                    torch_state_dict = torch.load(salm_model_path)
            else:
                torch_state_dict = torch.load(salm_model_path)['state_dict']
            model.setup_complete = False
            model.load_state_dict(torch_state_dict, strict=False)
            logging.info(f"loading from {cfg.model.get('salm_model_path')}: {torch_state_dict.keys()}")

        model.padded_vocab_size = cfg.model.s2s_vocab_size

        if cfg.model.get('megatron_amp_O2', False):
            base_model = model.model.module
        else:
            base_model = model.model

        base_model.extend_embedding(model.padded_vocab_size)
        # print out params in more details
        model.summarize(max_depth=2)

        cls.codec_model = codec_model
        cls.asr_model = asr_model
        cls.mos_model = mos_model
        return model

    # change to add one more dimension
    def _shift_labels_by_emb_len(self, labels, label_lens, emb_lens, max_len, pad_token=0):
        """Shift labels to the right by the length of the audio embeddings."""
        shifted_labels = []
        for label, label_len, emb_len in zip(labels, label_lens, emb_lens):
            shifted_label = torch.full([max_len, label[0].shape[0]], pad_token, device=label.device)
            shifted_label[emb_len : emb_len + label_len] = label[:label_len]
            shifted_labels.append(shifted_label)
        shifted_labels = torch.stack(shifted_labels, dim=0)
        return shifted_labels

    def inference_step(self, dataloader_iter, mode):
        """
        Used for validation and test steps, added postprocessing after calling self.predict_step().
        """
        # import pdb; pdb.set_trace()

        # Evaluation of multimodal data follows the same pattern as training except predict_step
        batch, batch_idx, dataloader_idx = next(dataloader_iter)
        data_cfg = self.cfg.data.validation_ds if mode == 'validation' else self.cfg.data.test_ds
        self._reconfigure_and_process_inference_batch(batch, data_cfg)
        # Meta data from dataset
        metadata = batch.get('metadata', [{}] * len(batch['tokens']))
        loss = super(MegatronGPTSFTModel, self).validation_step(itertools.chain([batch]), dataloader_idx)

        # We need _inference_config to get generation params
        # add_BOS and tokens_to_generate are set in dataset
        if self.get_inference_config() is None:
            logging.warning(f'inference_config is not set. Use default: {default_inference_config}')
            self.set_inference_config(inference_config=default_inference_config)
        self._inference_config['add_BOS'] = data_cfg.add_bos
        self._inference_config['tokens_to_generate'] = data_cfg.get('tokens_to_generate')

        output = self.predict_step(batch, batch_idx, dataloader_idx)

        inputs_text = (
            [self.tokenizer.ids_to_text(c.tolist()) for c in batch['instructions']]
            if batch['instructions'] is not None
            else [""] * len(batch['target_texts'])
        )
        labels_text = [self.tokenizer.ids_to_text(a.tolist()) for a in batch['target_texts']]
        # only do ids_to_text on the first channel which is text
        output['token_ids_text'] = (np.array(output['token_ids'])[:, :, 0]).tolist()
        preds_text = [
            self.tokenizer.ids_to_text(t[l.item() :][: data_cfg.get('tokens_to_generate')])
            for t, l in zip(output['token_ids_text'], batch['context_lengths'])
        ]

        if data_cfg.get("end_string", None):
            # sometimes data_cfg.end_string != self.tokenizer.ids_to_text(self.tokenizer.text_to_ids(data_cfg.end_string))
            # for example when data_cfg.end_string = "<end>", the end_string_re will start with " ?? "
            end_string_re = self.tokenizer.ids_to_text(self.tokenizer.text_to_ids(data_cfg.end_string))
            preds_text_cleaned = []
            labels_text_cleaned = []
            for p, l in zip(preds_text, labels_text):
                # remove end_string from the end of the string
                for es in [end_string_re, data_cfg.end_string]:
                    if p.endswith(es):
                        p = p[: -len(es)].strip()
                    if l.endswith(es):
                        l = l[: -len(es)].strip()
                preds_text_cleaned.append(p)
                labels_text_cleaned.append(l)
            # TODO: remove preds_text here since it is not used. the real preds_text is obtained by parse_decoder_outputs()
            preds_text = preds_text_cleaned
            labels_text = labels_text_cleaned

        if data_cfg.get("remove_text_pc", False):
            preds_text = [remove_punctuations(p.lower(), data_cfg.get("punctuations", None)) for p in preds_text]
            labels_text = [remove_punctuations(l.lower(), data_cfg.get("punctuations", None)) for l in labels_text]

        # if loss is nan, print the input, label and pred
        if loss.isnan():
            logging.info("++++++++++++++ NaN loss detected ++++++++++++++")
            for i in range(len(inputs_text)):
                logging.info(f"Input: `{inputs_text[i]}`")
                logging.info(f"Label: `{labels_text[i]}`")
                logging.info(f"Pred: `{preds_text[i]}`")
            logging.info("++++++++++++++++++++++++++++++++++++++++++++++++")

        outputs = {
            'loss': loss,
            'preds': output['token_ids'],
            'context_lengths': batch['context_lengths'],
            'labels': batch['answers'],  # [str]
            'labels_text': labels_text,  # [str]
            'inputs': inputs_text,  # [str]
            'metadata': metadata,  # [dict]
            'batch_idx': batch_idx,
        }

        if mode == 'validation':
            if len(self._validation_dl) > 1:
                # super().validation_step appends just loss to self.validation_step_outputs, replace the last appended loss with the outputs dict
                self.validation_step_outputs[dataloader_idx][-1] = outputs
            else:
                # super().validation_step appends just loss to self.validation_step_outputs, replace the last appended loss with the outputs dict
                self.validation_step_outputs[-1] = outputs
        else:
            if len(self._test_dl) > 1:
                self.test_step_outputs[dataloader_idx][-1] = outputs
            else:
                self.test_step_outputs[-1] = outputs
        return outputs




    def parse_decoder_outputs(
        self, input_decoder_output, text_separator, context_length, speech_pad_id=1001, speech_eos_id=1004
    ):
        # remove text context
        max_len = input_decoder_output.shape[0]
        if len(input_decoder_output.shape) == 1:
            return input_decoder_output, None
        decoder_output = input_decoder_output[-1:].tile([max_len, 1])
        decoder_output[: max_len - context_length] = input_decoder_output[context_length:]

        # Do not split because text and speech are now aligned
        # Split text and speech part based on the position of the first separator token
        # sep_pos = (decoder_output[:, 0] == text_separator).long()
        # if torch.any(sep_pos):
        #     first_sep_pos = torch.argmax(sep_pos)
        #     text_tokens = decoder_output[:first_sep_pos, 0]
        #     speech_tokens = decoder_output[first_sep_pos + 1 :, 1:]
        # else:
        #     text_tokens = decoder_output[:, 0]
        #     speech_tokens = decoder_output[:, 1:]
        text_tokens = decoder_output[:, 0]
        speech_tokens = decoder_output[:, 1:]
        # Get speech token ids
        if self.cfg.get('megatron_amp_O2', False):
            n_speech_codebooks = self.model.module.n_proj_heads - 1
        else:
            n_speech_codebooks = self.model.n_proj_heads - 1
        duplex_method = self.cfg.duplex_method
        if duplex_method != 'from_duplex':
            # Remove padded parts of speech tokens
            speech_eos_pos = torch.sum(speech_tokens == speech_eos_id, axis=1) == n_speech_codebooks
            speech_mask = torch.cumsum(speech_eos_pos, 0) == 0
            speech_tokens = speech_tokens[speech_mask]
        # Revert decoder output reduction
        new_shape = (
            speech_tokens.shape[0] * self.cfg.decoder_reduction_factor,
            speech_tokens.shape[1] // self.cfg.decoder_reduction_factor,
        )
        speech_tokens = speech_tokens.reshape(new_shape)
        if speech_tokens.shape[0] == 0:
            speech_tokens = torch.zeros([1, new_shape[1]]).long().cuda()
        return text_tokens.long(), speech_tokens.long()

    def decode_and_save_wavs(self, codec_model, codes_list, wav_dir, metadata_list):
        sample_rate = self.codec_sample_rate
        os.makedirs(wav_dir, exist_ok=True)
        wavs = []
        for codes, metadata in zip(codes_list, metadata_list):
            codes = torch.tensor(codes).to(codec_model.device).T
            codec_len = torch.Tensor([codes.shape[1]]).long().to(codec_model.device)
            wav, _ = codec_model.decode(tokens=codes.unsqueeze(0), tokens_len=codec_len)
            wav = wav[0]
            wavs.append(wav)
            sf.write(
                os.path.join(
                    wav_dir, re.sub("_repeat\d*", "", metadata['audio_filepath'].split('.wav')[0]) + ".gen.wav"
                ),
                wav.detach().cpu().numpy(),
                sample_rate,
            )

        return wavs

    def calculate_turn_taking(self, pred, answer, tolerance = 2):
        """
        Calculate whether the prediction is successful and the error between pred and answer.

        Args:
            pred: model prediction in list format.
            answer (torch.Tensor): 1D tensor containing the answer sequence, must contain exactly two 1s.
            tolerance: how much tokens can be pre earlier than the answer.
        Returns:
            (bool, tuple or None):
                - If successful, returns (True, (error1, error2)), where error1 and error2 are the index differences.
                - If failed, returns (False, None).
        """

        # Find bos indices in both pred and answer
        pred_indices = (torch.Tensor(pred).long()[:,0] == self.tokenizer.bos).nonzero(as_tuple=True)[0]
        answer_indices = (answer == self.tokenizer.bos).nonzero(as_tuple=True)[0]

        # Ensure answer contains exactly two bos
        if len(answer_indices) != 2:
            raise ValueError("This is not a 2-turn data.")

        # Check if pred contains exactly two bos
        if len(pred_indices) != 2:
            return False, None

        # Extract the first and second indices from pred and answer
        pred_first, pred_second = pred_indices[0].item(), pred_indices[1].item()
        answer_first, answer_second = answer_indices[0].item(), answer_indices[1].item()

        # Check if agent start to speak when user is speaking
        if pred_first < answer_first - tolerance or pred_second < answer_second - tolerance:
            return False, None

        # Calculate errors
        error1 = 0 if pred_first <= answer_first else pred_first - answer_first
        error2 = 0 if pred_second <= answer_second else pred_second - answer_second

        return True, (error1 + error2) / 2

    def inference_epoch_end(self, outputs, mode, data_cfg):
        # Parent class will handle logging of the loss.
        if not outputs or (all([not x for x in outputs])):
            return None

        if isinstance(outputs[0], dict):
            outputs = [outputs]

        averaged_loss = []
        # Log metrics for each provided validation/test dataset.
        for dataloader_idx, output in enumerate(outputs):

            if len(output) == 0:
                logging.warning(f"Empty output for dataloader_idx: {dataloader_idx}")
                continue
            # Expand on_validation_epoch_end from parent class MegatronGPTModel as on_validation_epoch_end doesnt take outputs arg
            loss_vals = [x['loss'] for x in output]
            if parallel_state.is_pipeline_last_stage():
                # only the last pipeline parallel stages return loss with their batch size
                if self.cfg.data.get('validation_drop_last', True):
                    loss = torch.stack(loss_vals).mean()
                else:
                    # Compute the avg loss by total_loss across all samples / total number of samples
                    total_loss_and_total_samples = torch.vstack(loss_vals).sum(axis=0)
                    avg_loss = total_loss_and_total_samples[0] / total_loss_and_total_samples[1]
                    loss = avg_loss.type(torch.float32).cuda()
            else:
                loss = torch.tensor(0.0, dtype=torch.float32).cuda()

            # we can only log on one rank if it is rank zero so we broadcast from last rank
            torch.distributed.broadcast(loss, get_last_rank())

            self.log('val_loss', loss, prog_bar=True, rank_zero_only=True, batch_size=1, sync_dist=True)

            # Determine the key used to log the loss based on the user provided name of the dataset or the dataloader index.
            loss_log_key = self._determine_log_key(data_cfg, dataloader_idx, "loss", mode)
            self.log(loss_log_key, loss, batch_size=1)
            averaged_loss.append(loss)

            # Gather the outputs object from all data parallel ranks since we are using the DistributedSampler which splits data across DDP ranks.
            gathered_outputs = [None for _ in range(parallel_state.get_data_parallel_world_size())]
            torch.distributed.all_gather_object(
                gathered_outputs,
                [
                    {
                        'preds': x['preds'],
                        'labels': x['labels'],
                        'inputs': x['inputs'],
                        'metadata': x['metadata'],
                        'context_lengths': x['context_lengths'],
                        'batch_idx': x['batch_idx'],
                        'labels_text': x['labels_text'],
                    }
                    for x in output
                ],
                group=parallel_state.get_data_parallel_group(),
            )

            # Remove duplicate examples due to distributed sampler.
            inp_label_set = set()
            deduplicated_outputs = {
                'preds': [],
                'labels': [],
                'inputs': [],
                'metadata': [],
                'speech_preds': [],
                'speech_answers': [],
                'text_answers': [],
                'batch_idx': [],
                'turn_taking': [],
                'turn_taking_error': [],
            }
            total_size = 0
            for rank in range(0, parallel_state.get_data_parallel_world_size()):
                for batch in gathered_outputs[rank]:
                    for pred, answer, input, metadata, pred_context_length, labels_text in zip(
                        batch['preds'],
                        batch['labels'],
                        batch['inputs'],
                        batch['metadata'],
                        batch['context_lengths'],
                        batch['labels_text'],
                    ):

                        turn_taking, turn_taking_error = self.calculate_turn_taking(pred, answer)

                        context_length = len(self.tokenizer.text_to_ids(input))
                        text_answer, speech_answer = self.parse_decoder_outputs(
                            answer,
                            self.tokenizer.eos_id,
                            context_length,
                            self.cfg.data.train_ds.speech_pad_id,
                            self.cfg.data.train_ds.speech_eos_id,
                        )
                        key = input + self.tokenizer.ids_to_text(text_answer) + str(metadata)
                        total_size += 1
                        if key not in inp_label_set:
                            inp_label_set.add(key)

                            text_pred, speech_pred = self.parse_decoder_outputs(
                                torch.Tensor(pred),
                                self.tokenizer.eos_id,
                                pred_context_length,
                                self.cfg.data.train_ds.speech_pad_id,
                                self.cfg.data.train_ds.speech_eos_id,
                            )

                            def normalize_text(text):
                                return text.strip().replace('⁇', '')

                            if speech_answer == None:
                                speech_answer = torch.zeros_like(speech_pred)
                            text_pred_text = self.tokenizer.ids_to_text(text_pred)
                            deduplicated_outputs['preds'].append(normalize_text(text_pred_text))
                            deduplicated_outputs['labels'].append(normalize_text(labels_text))
                            text_answer_text = self.tokenizer.ids_to_text(text_answer)
                            deduplicated_outputs['text_answers'].append(normalize_text(text_answer_text))
                            deduplicated_outputs['speech_preds'].append(speech_pred.cpu().numpy())
                            deduplicated_outputs['speech_answers'].append(speech_answer.cpu().numpy())
                            deduplicated_outputs['inputs'].append(input)
                            deduplicated_outputs['metadata'].append(metadata)
                            deduplicated_outputs['batch_idx'].append(batch['batch_idx'])
                            deduplicated_outputs['turn_taking'].append(turn_taking)
                            if turn_taking:
                                deduplicated_outputs['turn_taking_error'].append(turn_taking_error)


            # Compute metric score
            metric_name = self.val_metric_name if mode == 'validation' else self.test_metric_name
            metric = self.val_metric if mode == 'validation' else self.test_metric
            averaged_metric = [[] for _ in range(len(metric_name))]
            output_dir = data_cfg.get("output_dir", "./")
            run_codec = any(("asr" in metric_name or "mos" in metric_name) for metric_name in metric_name)
            run_asr = any("asr" in metric_name for metric_name in metric_name)
            run_mos = any("mos" in metric_name for metric_name in metric_name)

            # TODO: move the following model init code to init() function
            if run_codec:
                self.additional_models['codec_model'] = self.codec_model
                assert 'codec_model' in self.additional_models
                codec_model = self.additional_models['codec_model']
                codec_model.to(self.device)
                codec_model.eval()

                with torch.no_grad():
                    logging.info(f"Decoding and saving audio")
                    pred_wavs = self.decode_and_save_wavs(
                        codec_model,
                        deduplicated_outputs['speech_preds'],
                        os.path.join(output_dir, "wav", "pred"),
                        deduplicated_outputs['metadata'],
                    )
                    answer_wavs = self.decode_and_save_wavs(
                        codec_model,
                        deduplicated_outputs['speech_answers'],
                        os.path.join(output_dir, "wav", "answer"),
                        deduplicated_outputs['metadata'],
                    )

            if run_asr:
                self.additional_models['asr_model'] = self.asr_model
                assert 'asr_model' in self.additional_models
                asr_model = self.additional_models['asr_model']

                with torch.no_grad():
                    logging.info(f"Running ASR on speech preds")
                    asr_batch_size = min(64, len(pred_wavs))
                    speech_preds_transcribed = asr_model.transcribe(pred_wavs, batch_size=asr_batch_size)
                    speech_answers_transcribed = asr_model.transcribe(answer_wavs, batch_size=asr_batch_size)
                    deduplicated_outputs['speech_preds_transcribed'] = speech_preds_transcribed
                    deduplicated_outputs['speech_answers_transcribed'] = speech_answers_transcribed

            if run_mos:
                self.additional_models['squim_mos_model'] = self.mos_model
                assert 'squim_mos_model' in self.additional_models
                squim_mos_model = self.additional_models['squim_mos_model']
                codec_sample_rate = self.codec_sample_rate

                with torch.no_grad():
                    logging.info(f"Running MOS prediction")

                    pred_wavs_resampled = [
                        torchaudio.functional.resample(wav, codec_sample_rate, 16000).unsqueeze(0) for wav in pred_wavs
                    ]
                    answer_wavs_resampled = [
                        torchaudio.functional.resample(wav, codec_sample_rate, 16000).unsqueeze(0)
                        for wav in answer_wavs
                    ]
                    squim_mos_scores = [
                        squim_mos_model(pred_wav, answer_wav)
                        for pred_wav, answer_wav in zip(pred_wavs_resampled, answer_wavs_resampled)
                    ]
                    deduplicated_outputs['mos_scores'] = squim_mos_scores

            if self.global_rank == 0:
                for (
                    labels,
                    text_answer_text,
                    preds,
                    speech_preds_transcribed,
                    speech_answer,
                    speech_pred,
                    inputs,
                    batch_idx,
                    speech_answers_transcribed,
                ) in zip(
                    deduplicated_outputs['labels'],
                    deduplicated_outputs['text_answers'],
                    deduplicated_outputs['preds'],
                    deduplicated_outputs['speech_preds_transcribed'],
                    deduplicated_outputs['speech_answers'],
                    deduplicated_outputs['speech_preds'],
                    deduplicated_outputs['inputs'],
                    deduplicated_outputs['batch_idx'],
                    deduplicated_outputs['speech_answers_transcribed'],
                ):
                    if (
                        data_cfg.get("log_every_n_steps", None) is not None
                        and batch_idx % data_cfg.log_every_n_steps == 0
                    ):
                        logging.info(f"Input: `{inputs}`")
                        logging.info(f"Label: `{labels}` text_answer_text: `{text_answer_text}`")
                        logging.info(f"Pred: `{preds}`")
                        logging.info(f"speech_preds_transcribed: `{speech_preds_transcribed}`")
                        logging.info(f"speech_answers_transcribed: `{speech_answers_transcribed}`")
                        logging.info(f"Speech out len: pred {speech_pred.shape} label {speech_answer.shape}")


            turn_taking_sr = sum(deduplicated_outputs['turn_taking']) / len(deduplicated_outputs['turn_taking'])
            turn_taking_er = sum(deduplicated_outputs['turn_taking_error']) / len(deduplicated_outputs['turn_taking_error']) if deduplicated_outputs['turn_taking_error'] else torch.inf


            logging.info(f'The success rate of turn taking is: {turn_taking_sr}')
            logging.info(f'The average turn taking latency is: {turn_taking_er}')
            # Compute metric score
            for metric_name, metric_fn, averaged_metric in zip(metric_name, metric, averaged_metric):
                if metric_name != 'loss':
                    metric_log_key = self._determine_log_key(data_cfg, dataloader_idx, metric_name, mode)
                    labels = deduplicated_outputs['labels']
                    # sacrebleu.corpus_bleu is commonly used which does not share
                    # the same interface as other metrics. We handle it separately.
                    text_preds = deduplicated_outputs['preds']



                    if "asr-" in metric_name:
                        text_preds = deduplicated_outputs['speech_preds_transcribed']


                    text_metric_name = metric_name.replace("asr-", "")

                    if text_metric_name == 'bleu':  # asr-bleu, bleu
                        metric_result = torch.Tensor([sacrebleu.corpus_bleu(text_preds, [labels]).score]).to(
                            self.device
                        )
                    elif text_metric_name == 'wer':  # asr-wer, wer
                        for pred, label in zip(text_preds, labels):
                            _ = metric_fn(pred, label)

                        metric_result = metric_fn.compute()
                        metric_fn.reset()
                    elif metric_name == 'mos':
                        metric_result = sum(deduplicated_outputs['mos_scores']) / len(
                            deduplicated_outputs['mos_scores']
                        )

                    else:
                        for pred, label in zip(deduplicated_outputs['preds'], labels):
                            _ = metric_fn(pred, label)

                        metric_result = metric_fn.compute()
                        metric_fn.reset()

                    self.log(metric_log_key, metric_result.item(), sync_dist=True)
                    logging.info(f"{mode} {metric_name}: {metric_result.item()}")

                    averaged_metric.append(metric_result)

            # Write predictions to file
            if self.global_rank == 0 and data_cfg.get("write_predictions_to_file", False):
                logging.info(
                    f"Total deduplicated inference data size: {total_size} to {len(deduplicated_outputs['inputs'])}"
                )

                # Check if the user provided a prefix path to the file(s) they want to write.
                if not hasattr(data_cfg, "output_file_path_prefix") or data_cfg.output_file_path_prefix is None:
                    raise ValueError(
                        f"Cannot write predictions to file when output_file_path_prefix is not set or present in the yaml config file."
                    )
                filename_log_key = self._determine_log_key(data_cfg, dataloader_idx, None, mode)
                output_dir = data_cfg.get("output_dir", "./")
                self.write_predictions_to_file(
                    deduplicated_outputs, f"{data_cfg.output_file_path_prefix}_{filename_log_key}", output_dir
                )

            torch.distributed.barrier(group=parallel_state.get_data_parallel_group())
            outputs[dataloader_idx].clear()  # free memory

        # Logging of the averaged metrics:
        averaged_loss = sum(averaged_loss) / len(averaged_loss)
        averaged_metric = sum(averaged_metric) / len(averaged_metric) if len(averaged_metric) > 0 else None
        averaged_loss = averaged_loss.to(self.device)
        if averaged_metric is not None:
            averaged_metric = averaged_metric.to(self.device)

        # Handle case where metrics can be nan or inf. This can break checkpoint save/load.
        if averaged_metric is not None and (torch.isinf(averaged_metric) or torch.isnan(averaged_metric)):
            app_state = AppState()
            monitor_mode = app_state.checkpoint_callback_params.mode
            assert monitor_mode in ['min', 'max']
            averaged_metric = 0.0 if monitor_mode == 'max' else 1e5

        if mode == 'validation':
            self.log("validation_loss", averaged_loss, batch_size=1, sync_dist=True)
            if averaged_metric is not None:
                self.log(f"validation_{self.val_metric_name}", averaged_metric, sync_dist=True, batch_size=1)
        elif mode == 'test':
            self.log("test_loss", averaged_loss, batch_size=1, sync_dist=True)
            if averaged_metric is not None:
                self.log(f"test_{self.test_metric_name}", averaged_metric, sync_dist=True, batch_size=1)

        # Merge the functionality of previous on_inference_epoch_end() within inference_epoch_end() func here
        app_state = AppState()
        self._restore_activation_checkpointing_args()
        if hasattr(self, "_train_ds"):
            reconfigure_num_microbatches_calculator(
                rank=app_state.global_rank,
                rampup_batch_size=None,
                global_batch_size=self.cfg.data.train_ds.global_batch_size,
                micro_batch_size=self.cfg.data.train_ds.micro_batch_size,
                data_parallel_size=parallel_state.get_data_parallel_world_size(),
            )
        # When running `trainer.validate()`, the training dataset is not available.
        else:
            logging.warning('No training data found, reconfiguring microbatches based on validation batch sizes.')
            reconfigure_num_microbatches_calculator(
                rank=app_state.global_rank,
                rampup_batch_size=None,
                global_batch_size=data_cfg.global_batch_size,
                micro_batch_size=data_cfg.micro_batch_size,
                data_parallel_size=parallel_state.get_data_parallel_world_size(),
            )

        return averaged_loss, averaged_metric

    # consistent with speech models
    @rank_zero_only
    def write_predictions_to_file(self, outputs, output_file_path_prefix, output_dir):
        os.makedirs(output_dir, exist_ok=True)
        for folder_name in ['speech_pred', 'speech_answer', 'speaker_contexts']:
            os.makedirs(os.path.join(output_dir, 'npy', folder_name), exist_ok=True)
        output_file_path = output_file_path_prefix + "_inputs_preds_labels.jsonl"
        output_file_path = os.path.join(output_dir, output_file_path)
        with open(output_file_path, "w") as f_json:
            assert (
                len(outputs['inputs']) == len(outputs['preds']) == len(outputs['labels']) == len(outputs['metadata'])
            )
            for i, p, l, m, speech_preds_transcribed, speech_answers_transcribed in zip(
                outputs['inputs'],
                outputs['preds'],
                outputs['labels'],
                outputs['metadata'],
                outputs['speech_preds_transcribed'],
                outputs['speech_answers_transcribed'],
            ):
                json_string = {
                    'input': i,
                    'pred_text': p,
                    'text': l,
                    'speech_preds_transcribed': speech_preds_transcribed,
                    'speech_answers_transcribed': speech_answers_transcribed,
                }
                for k, v in m.items():
                    if k not in json_string:
                        json_string[k] = v
                f_json.write(json.dumps(json_string) + '\n')

        logging.info(f'Predictions saved to {output_file_path}')

    def de_concat_multiproj_logits(self, logits):
        logits_list = []
        prev = 0

        if self.cfg.get('megatron_amp_O2', False):
            base_model = self.model.module
        else:
            base_model = self.model

        for i in base_model.proj_head_dims:
            logits_list.append(logits[:, prev : prev + i])
            prev += i
        return logits_list

    def setup_metric(self, data_cfg):
        metric_name = "exact_string_match"
        if not hasattr(data_cfg, "metrics"):
            metrics = [(MetricStringToTorchMetric["exact_string_match"], "exact_string_match")]
        else:
            metrics = []
            for metric in data_cfg.metrics:
                if not hasattr(metric, "name"):
                    raise ValueError("Metric name is not provided in the metric config.")
                base_metric_name = metric.name.replace("asr-", "")
                if metric.name == "loss" or metric.name == "mos":
                    metrics.append((None, metric.name))
                    continue
                if base_metric_name not in MetricStringToTorchMetric:
                    raise KeyError(
                        f"{metric.name} is not supported. List of supported metrics: {MetricStringToTorchMetric.keys()}"
                    )
                if base_metric_name in self._metrics_require_string2category_map:
                    if metric.average is None:
                        raise ValueError(
                            f"{metric.name} requires specifying whether you want to compute a micro or macro average. Found None."
                        )
                if (
                    metric.get('labels_are_strings', False)
                    and base_metric_name in self._metrics_require_string2category_map
                ):
                    if metric.num_classes is None:
                        raise ValueError(
                            "Number of classes is not provided in the metric section within the data config. "
                            f"Please provide the number of classes in the data config to use the {metric.name} metric."
                        )
                    if metric.get('class_labels', None) is None or not isinstance(
                        metric.get('class_labels', None), ListConfig
                    ):
                        raise ValueError(
                            "Class labels are not provided properly in the metric section witnin the data config. "
                            f"Please provide the class labels as a list of strings in the data config to use the {metric.name} metric."
                        )
                    if len(metric.get('class_labels', None)) != metric.num_classes:
                        raise ValueError(
                            f"Number of class labels {len(metric.get('class_labels', None))} does not match `num_classes` : {metric.num_classes}"
                        )

                metric_cls = MetricStringToTorchMetric[base_metric_name]
                if base_metric_name not in TextMetricsSet:
                    metric_fn = metric_cls(**data_cfg.metric)
                else:
                    metric_fn = metric_cls()
                metrics.append((metric_fn, metric.name))
        return zip(*metrics)

    def __init__(self, cfg: DictConfig, trainer: Trainer):
        self.cfg = cfg
        self.additional_models = {}
        self.extract_codec_on_the_fly = cfg.get('extract_codec_on_the_fly', False)
        self.codec_model_downsampling_factor = cfg.get('codec_model_downsampling_factor', 1023.5)
        self.codec_sample_rate = cfg.data.train_ds.get("codec_sample_rate", 22050)

        super().__init__(cfg, trainer)

    def _get_codec_embeddings(self, audio_signal, audio_signal_length):
        """Get codec embeddings for the input audio signal."""
        if 'codec_model' not in self.additional_models:
            self.additional_models['codec_model'] = self.codec_model
            self.additional_models['codec_model'].to(self.device)
            self.additional_models['codec_model'].eval()
        codec_model = self.additional_models['codec_model']
        codec_model.eval()
        with torch.no_grad():
            original_codec_codes, _ = codec_model.encode(audio=audio_signal, audio_len=audio_signal_length)
            original_codec_codes = original_codec_codes.transpose(1, 2)
        out_codec_codes = []
        out_codec_lens = []
        n_speech_codebooks = original_codec_codes.shape[-1]
        decoder_reduction_factor = self.cfg.get("decoder_reduction_factor", 1)
        speech_pad_id = self.cfg.data.train_ds.speech_pad_id
        padded_original_codec_codes = torch.cat(
            [
                original_codec_codes,
                torch.ones([original_codec_codes.shape[0], decoder_reduction_factor, n_speech_codebooks]).long().cuda()
                * speech_pad_id,
            ],
            axis=1,
        )
        for sidx in range(audio_signal.shape[0]):
            codec_len = min(
                torch.ceil(audio_signal_length[sidx] / self.codec_model_downsampling_factor / decoder_reduction_factor)
                .int()
                .to(self.device),
                math.ceil(original_codec_codes[sidx].shape[0] / decoder_reduction_factor),
            )
            out_codec_codes.append(
                padded_original_codec_codes[sidx, : codec_len * decoder_reduction_factor]
                .reshape((-1, n_speech_codebooks * decoder_reduction_factor))
                .to(self.device)
            )
            out_codec_lens.append(codec_len)

        return out_codec_codes, out_codec_lens

    def get_duration_by_steps(self, steps):
        codec_model_downsampling_factor = self.codec_model_downsampling_factor
        decoder_reduction_factor = self.cfg.get("decoder_reduction_factor", 1)
        codec_sample_rate = self.codec_sample_rate
        seconds = steps * codec_model_downsampling_factor / codec_sample_rate * decoder_reduction_factor
        return seconds, int(seconds * codec_sample_rate)

    def get_step_from_audio_len(self, audio_len):
        decoder_reduction_factor = self.cfg.get("decoder_reduction_factor", 1)
        return torch.ceil(audio_len / self.codec_model_downsampling_factor / decoder_reduction_factor).int() - 1

    def prepare_llm_input_duplex_from_multiturn(self, audio_batch):
        codec_sample_rate = self.codec_sample_rate
        decoder_reduction_factor = self.cfg.get("decoder_reduction_factor", 1)
        # make the following to be one decoding step so as to easier replace with speech bos token and eos token
        duplex_inject_silence_second = (
            self.codec_model_downsampling_factor / codec_sample_rate * decoder_reduction_factor
        )
        silence = int(codec_sample_rate * duplex_inject_silence_second)
        user_signal = audio_batch['audio_signal']
        user_signal_length = audio_batch['audio_signal_length']

        def resample(audio, audio_lens, orig_sample_rate, target_sample_rate):
            audio = torchaudio.functional.resample(audio, orig_sample_rate, target_sample_rate)
            audio_lens = (audio_lens * (target_sample_rate / orig_sample_rate)).int()
            return audio, audio_lens

        if 'target_texts_merge' not in audio_batch:  # create duplex data from single turn
            # this branch is not used anymore; duplex data should go to else:
            labels, loss_mask = (
                audio_batch['labels'],
                audio_batch['loss_mask'],
            )
            context_lengths = audio_batch['context_lengths']

            assert self.extract_codec_on_the_fly
            agent_signal = audio_batch['answer_audio']
            agent_signal_length = audio_batch['answer_audio_lens']

            if self.perception.cfg.preprocessor.sample_rate != codec_sample_rate:
                user_signal, user_signal_length = resample(
                    user_signal,
                    user_signal_length,
                    self.perception.cfg.preprocessor.sample_rate,
                    codec_sample_rate,
                )

            new_user_signal = []
            new_agent_signal = []
            new_user_signal_length = []
            new_agent_signal_length = []
            silence_value = 0
            shift_text_channel_len = []
            agent_bos_eos_step = []
            for user, agent, user_len, agent_len in zip(
                user_signal, agent_signal, user_signal_length, agent_signal_length
            ):
                user = user[:user_len]
                agent = agent[:agent_len]
                # user, silence, agent, silence -> user, bos, agent, eos
                # TODO: above design means that in real/synthetic data, we need to mark bos and eos timestamp of agent responses
                silence_piece = torch.full([silence], silence_value).cuda()
                new_user_signal.append(
                    torch.cat([user, silence_piece, torch.ones_like(agent) * silence_value, silence_piece], dim=0)
                )
                new_agent_signal.append(
                    torch.cat([torch.ones_like(user) * silence_value, silence_piece, agent, silence_piece], dim=0)
                )
                duplex_len = user_len + silence + agent_len + silence
                # make bos step -1 to be safe for silence+speech boundary
                agent_bos_eos_step.append(
                    [self.get_step_from_audio_len(user_len + silence) - 1, self.get_step_from_audio_len(duplex_len)]
                )
                new_user_signal_length.append(duplex_len)
                new_agent_signal_length.append(duplex_len)
            new_user_signal = pad_sequence(new_user_signal, batch_first=True)
            new_agent_signal = pad_sequence(new_agent_signal, batch_first=True)
            new_user_signal_length = torch.Tensor(new_user_signal_length).long().cuda()
            new_agent_signal_length = torch.Tensor(new_agent_signal_length).long().cuda()
            if self.perception.cfg.preprocessor.sample_rate != codec_sample_rate:
                new_user_signal, new_user_signal_length = resample(
                    new_user_signal,
                    new_user_signal_length,
                    codec_sample_rate,
                    self.perception.cfg.preprocessor.sample_rate,
                )
        else:  # real duplex data read from dataloader
            new_user_signal = audio_batch['audio_signal']
            new_user_signal_length = audio_batch['audio_signal_length']
            new_agent_signal = audio_batch['answer_audio']
            new_agent_signal_length = audio_batch['answer_audio_lens']
            loss_mask = None
            duplex_method = self.cfg.duplex_method
            assert duplex_method == "from_duplex"

        # [b, t, c]
        encoded, encoded_len = self.perception(
            input_signal=new_user_signal,
            input_signal_length=new_user_signal_length,
            processed_signal=None,
            processed_signal_length=None,
        )

        answer_codecs, answer_codecs_lens = self._get_codec_embeddings(
            new_agent_signal, new_agent_signal_length
        )  # list, list

        answer_codecs_lens = torch.Tensor(answer_codecs_lens).long().cuda()
        assert all(torch.isclose(answer_codecs_lens, encoded_len, atol=1))
        encoded_len = answer_codecs_lens
        if 'answer_features_lens' in audio_batch:
            assert 'target_texts_merge' not in audio_batch
            prev_answer_features_lens = (
                torch.ceil(
                    agent_signal_length / self.codec_model_downsampling_factor / decoder_reduction_factor
                ).long()
                + 1
            )  # bos
            assert all(prev_answer_features_lens == audio_batch['answer_features_lens'])
            shift_text_channel_len = answer_codecs_lens - prev_answer_features_lens - 2  # 2 is for bos and eos

        new_loss_mask = []
        all_channels = []
        for i, answer_codec in enumerate(answer_codecs):
            if 'target_texts_merge' in audio_batch:
                text_channel = audio_batch['target_texts_merge'][i]
                sliced_text_channel = text_channel[: answer_codec.shape[0]].unsqueeze(-1)
                answer_codec = torch.where(
                    sliced_text_channel == self.tokenizer.bos_id, self.cfg.data.train_ds.speech_bos_id, answer_codec
                )
                answer_codec = torch.where(
                    sliced_text_channel == self.tokenizer.eos_id, self.cfg.data.train_ds.speech_eos_id, answer_codec
                )
            else:
                # this branch is not used anymore
                # mask bos and eos following timestamp or synthetic data mark
                answer_codec[agent_bos_eos_step[i][0]] = self.cfg.data.train_ds.speech_bos_id
                answer_codec[agent_bos_eos_step[i][1]] = self.cfg.data.train_ds.speech_eos_id
                pad_id = self.tokenizer.pad_id if self.tokenizer.pad_id > 0 else self.tokenizer.unk_id
                base_length = -1 + context_lengths[i]
                text_channel = torch.cat(
                    [
                        torch.full([shift_text_channel_len[i], 1], pad_id).cuda(),
                        torch.full([1, 1], self.tokenizer.bos_id).cuda(),
                        labels[i, base_length:, :1],
                    ],
                    dim=0,
                )
                sliced_text_channel = text_channel[: answer_codec.shape[0]]

            if getattr(self.cfg, 'speech_delay', False):
                text_len, text_vocab = sliced_text_channel.shape
                speech_len, speech_vocab = answer_codec.shape
                assert text_len == speech_len
                speech_pad_id = self.cfg.data.train_ds.speech_unk_id
                text_pad_id = self.tokenizer.eos_id
                answer_codec_padded = torch.full(
                    (self.cfg.speech_delay, speech_vocab), speech_pad_id, device=answer_codec.device
                )
                answer_codec_shifted = torch.cat([answer_codec_padded, answer_codec], dim=0)[:speech_len, :]
                sliced_text_channel_padded = torch.full(
                    (self.cfg.speech_delay, text_vocab), text_pad_id, device=sliced_text_channel.device
                )
                sliced_text_channel_extended = torch.cat([sliced_text_channel, sliced_text_channel_padded], dim=0)[
                    :speech_len, :
                ]
                combined_channels = torch.cat([sliced_text_channel_extended, answer_codec_shifted], dim=-1)
                all_channels.append(combined_channels)
            else:
                # checked text_channel, loss_mask;  checked injecting bos and eos properly to control turn taking in inference
                all_channels.append(torch.cat([sliced_text_channel, answer_codec], dim=-1))

            if loss_mask is not None:
                cur_loss_mask = torch.cat(
                    [torch.zeros([shift_text_channel_len[i], loss_mask.shape[-1]]).cuda(), loss_mask[i, base_length:]],
                    dim=0,
                )
                new_loss_mask.append(cur_loss_mask[: answer_codec.shape[0]])
        all_channels = pad_sequence(all_channels, batch_first=True)

        input_ids = all_channels[:, :-1]
        encoded = encoded[:, : input_ids.shape[1]]
        encoder_length = encoded_len - 1
        labels = all_channels[:, 1:]
        if loss_mask is not None:
            loss_mask = pad_sequence(new_loss_mask, batch_first=True)
            assert loss_mask.shape == labels.shape
            if self.cfg.get('duplex_loss_on_all_steps', False):
                loss_mask = torch.ones_like(labels)  # include loss on silence too
        elif 'target_texts_merge' in audio_batch:
            loss_mask = torch.ones_like(labels)
            assert self.cfg.get(
                'duplex_loss_on_all_steps', False
            ), "only support duplex_loss_on_all_steps in real duplex data read from dataloader"
        assert labels.shape[1] == encoded.shape[1]
        # lookup input_ids
        if self.cfg.get('megatron_amp_O2', False):
            base_module = self.model.module
        else:
            base_module = self.model
        lm_embedding = (
            base_module.language_model.embedding if hasattr(base_module, 'language_model') else base_module.embedding
        )
        input_embeds = lm_embedding.word_embeddings(input_ids)
        # merge with encoded
        encoder_input = input_embeds + encoded * self.cfg.get("duplex_user_channel_weight", 0.3)
        attention_mask = self._create_attention_mask(encoder_input)
        if not hasattr(lm_embedding, 'transpose_batch_sequence') or lm_embedding.transpose_batch_sequence:
            encoder_input = encoder_input.transpose(0, 1).contiguous()

        return encoder_input, attention_mask, labels, loss_mask, (encoded, encoder_length)

    def prepare_llm_input(self, audio_batch):
        # handle duplex and singleturn s2s
        assert self.perception.cfg.preprocessor.sample_rate == self.cfg.data.train_ds.sample_rate
        duplex_method = self.cfg.duplex_method

        if duplex_method == 'from_duplex':
            # duplex data should go here
            assert 'target_texts_merge' in audio_batch
            return self.prepare_llm_input_duplex_from_multiturn(audio_batch)
        # the following branches are not used anymore
        elif duplex_method == 'from_multiturn':
            return self.prepare_llm_input_duplex_from_multiturn(audio_batch)
        elif duplex_method == None:
            pass
        else:
            raise ValueError(f"Unknown duplex method: {duplex_method}")

        # the following branch is used in single turn and multiturn but not duplex
        input_signal = audio_batch['audio_signal']
        logging.debug(f'input_signal.shape: {input_signal.shape}')
        input_signal_length = audio_batch['audio_signal_length']

        input_ids, input_length, labels, loss_mask = (
            audio_batch['tokens'],
            audio_batch['tokens_length'],
            audio_batch['labels'],
            audio_batch['loss_mask'],
        )
        context_lengths = audio_batch['context_lengths']

        if self.extract_codec_on_the_fly:
            answer_signal = audio_batch['answer_audio']
            answer_signal_length = audio_batch['answer_audio_lens']
            target_text_lengths = audio_batch['target_text_lengths']

            answer_codecs, answer_codecs_lens = self._get_codec_embeddings(
                answer_signal, answer_signal_length
            )  # list, list
            for i, answer_codec in enumerate(answer_codecs):
                base_length = target_text_lengths[i] + context_lengths[i]
                input_ids[i, base_length + 1 : base_length + 1 + answer_codecs_lens[i], 1:] = answer_codec
                labels[i, base_length : base_length + answer_codecs_lens[i], 1:] = answer_codec

        num_audios = audio_batch.get("num_audios", None)
        context_start_idx = audio_batch.get("context_start_idx", None)

        # [b, t, c]
        encoded, encoded_len = self.perception(
            input_signal=input_signal,
            input_signal_length=input_signal_length,
            processed_signal=None,
            processed_signal_length=None,
        )

        logging.debug(f'encoded.shape: {encoded.shape}')
        logging.debug(f'encoded_len.shape: {encoded_len.shape}')
        logging.debug(f'num_audios: {num_audios}')
        if num_audios is not None:
            # split the encoded and encoded_len by num_audios, used when there're multiple audio files per sample
            encoded = encoded.split(num_audios.tolist())
            encoded_len = encoded_len.split(num_audios.tolist())

        encoder_input, attention_mask, encoder_length, _, encoder_max_length = self.inject_perception_input(
            encoded, encoded_len, input_ids, input_length, context_start_idx
        )
        if num_audios is not None:
            # sum up the audio_feat_lens for each sample in the batch
            encoded_len = torch.stack([torch.sum(lens) for lens in encoded_len])

        # Shift labels to the right
        labels = self._shift_labels_by_emb_len(labels, input_length, encoded_len, encoder_max_length, pad_token=0)
        # Loss mask where answer tokens are 1.0 and all other tokens are 0.0
        loss_mask = self._shift_labels_by_emb_len(
            loss_mask, input_length, encoded_len, encoder_max_length, pad_token=0
        )
        # return if it is single turn or duplex
        if (
            all(audio_batch['num_turns'] == 2) or 'target_texts_merge' in audio_batch
        ):  # real duplex data read from dataloader
            return encoder_input, attention_mask, labels, loss_mask, encoder_length
        # special logic to handle multiturn half-duplex s2s
        # use num_turns to recover multiturn format and then merge them back to one sequence as LLM input/output
        new_encoder_input = []
        new_labels = []
        new_loss_mask = []
        new_encoder_length = []
        cnt = 0
        for num_turns in audio_batch['num_turns']:
            tmp_encoder_input = []
            tmp_labels = []
            tmp_loss_mask = []
            tmp_encoder_length = []
            for i in range(0, num_turns, 2):
                input_len = encoder_length[cnt]
                if i != num_turns - 2:  # last turn
                    input_len -= 1  # remove the last token as it is eos between the turns
                tmp_encoder_input.append(encoder_input.transpose(0, 1)[cnt][:input_len])
                tmp_labels.append(labels[cnt][:input_len])
                tmp_loss_mask.append(loss_mask[cnt][:input_len])
                tmp_encoder_length.append(input_len)
                cnt += 1
            new_encoder_input.append(torch.cat(tmp_encoder_input, dim=0))
            new_encoder_length.append(sum(tmp_encoder_length))
            new_labels.append(torch.cat(tmp_labels, dim=0))
            new_loss_mask.append(torch.cat(tmp_loss_mask, dim=0))
        new_encoder_input = pad_sequence(new_encoder_input, batch_first=True)
        new_encoder_length = torch.Tensor(new_encoder_length).long()
        new_labels = pad_sequence(new_labels, batch_first=True)
        new_loss_mask = pad_sequence(new_loss_mask, batch_first=True)
        assert cnt == encoder_length.shape[0]
        new_attention_mask = self._create_attention_mask(new_encoder_input)
        return (
            new_encoder_input.transpose(0, 1).contiguous(),
            new_attention_mask,
            new_labels,
            new_loss_mask,
            new_encoder_length,
        )

    @classmethod
    def get_codec_models_and_configs(cls, cfg):
        pretrained_codec_model = cfg.model.get("codec_model_path", None)
        pretrained_codec_model_class = cfg.model.get(
            "pretrained_codec_model_target", "nemo.collections.tts.models.audio_codec.AudioCodecModel"
        )

        model_class = hydra.utils.get_class(pretrained_codec_model_class)
        if pretrained_codec_model.endswith('.nemo'):
            logging.info(f'Loading pretrained codec model from local file: {pretrained_codec_model}')
            codec_model = model_class.restore_from(pretrained_codec_model, map_location='cpu')
        else:
            logging.info(f'Loading pretrained codec model from NGC: {pretrained_codec_model}')
            codec_model = model_class.from_pretrained(pretrained_codec_model, map_location='cpu')
        return codec_model, codec_model.cfg

    @classmethod
    def get_asr_models_and_configs(cls, cfg):

        pretrained_asr_model = cfg.model.get("asr_model_path", None)
        pretrained_asr_model_class = cfg.model.get(
            "pretrained_asr_model_target", "nemo.collections.asr.models.ASRModel"
        )

        model_class = hydra.utils.get_class(pretrained_asr_model_class)
        if pretrained_asr_model.endswith('.nemo'):
            logging.info(f'Loading pretrained codec model from local file: {pretrained_asr_model}')
            asr_model = model_class.restore_from(pretrained_asr_model, map_location='cpu')
        else:
            logging.info(f'Loading pretrained asr model from NGC: {pretrained_asr_model}')
            asr_model = model_class.from_pretrained(pretrained_asr_model, map_location='cpu')
        return asr_model, asr_model.cfg

    @classmethod
    def get_mos_models_and_configs(cls, cfg):

        squim_mos_model = SQUIM_SUBJECTIVE.get_model()
        return squim_mos_model

    def setup_optimizer_param_groups(self):
        super().setup_optimizer_param_groups()
        freeze_llm = self.cfg.get('freeze_llm', True)
        if freeze_llm:
            # needs to be updated since vocab is changed
            for param in self.model.embedding.parameters():
                param.requires_grad = True
            for param in self.model.output_layer.parameters():
                param.requires_grad = True
